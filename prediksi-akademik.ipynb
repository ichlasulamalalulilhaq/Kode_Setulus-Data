{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12408411,"sourceType":"datasetVersion","datasetId":7825373}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# OPTIMIZED STUDENT GRADE PREDICTION MODEL - MAXIMUM ACCURACY\n\n# Installation\n!pip install xgboost==1.7.6 imbalanced-learn scikit-learn==1.3.2 pandas numpy matplotlib seaborn optuna\n\nimport pandas as pd\nimport joblib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nimport warnings\nimport os # Ditambahkan untuk mengelola file dan direktori\n\nwarnings.filterwarnings('ignore')\n\nprint(\"Tahap 1: Library berhasil dipersiapkan\")\n\n# DATA LOADING & COMPREHENSIVE PREPROCESSING\n\n# Load dataset\n# Pastikan path file ini benar\ndf = pd.read_csv(\"/kaggle/input/student/Students Performance Dataset.csv\")\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Missing values: {df.isnull().sum().sum()}\")\n\n# Analisis distribusi target\nprint(\"\\nDistribusi Grade:\")\ngrade_dist = df['Grade'].value_counts().sort_index()\nprint(grade_dist)\n\n# Data cleaning\ncolumns_to_drop = ['Student_ID', 'First_Name', 'Last_Name', 'Email']\ndf_cleaned = df.drop(columns=columns_to_drop)\n\n# Advanced missing value handling\nprint(\"\\nHandling missing values...\")\nfor col in df_cleaned.columns:\n    if df_cleaned[col].isnull().sum() > 0:\n        if df_cleaned[col].dtype in ['int64', 'float64']:\n            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n        else:\n            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0] if not df_cleaned[col].mode().empty else 'Unknown')\n\n# Enhanced Feature Engineering\ndf_engineered = df_cleaned.copy()\ndf_engineered['academic_performance'] = (df_engineered['Midterm_Score'] * 0.3 + df_engineered['Final_Score'] * 0.4 + df_engineered['Projects_Score'] * 0.2 + df_engineered['Assignments_Avg'] * 0.1)\ndf_engineered['engagement_score'] = (df_engineered['Attendance (%)'] * 0.35 + df_engineered['Participation_Score'] * 0.4 + df_engineered['Assignments_Avg'] * 0.25)\ndf_engineered['performance_consistency'] = 1 - np.abs((df_engineered['Midterm_Score'] - df_engineered['Final_Score']) / (df_engineered['Midterm_Score'] + df_engineered['Final_Score'] + 1e-8))\ndf_engineered['study_efficiency'] = (df_engineered['academic_performance'] / (df_engineered['Study_Hours_per_Week'] + 1e-8))\ndf_engineered['stress_resilience'] = (df_engineered['academic_performance'] * (11 - df_engineered['Stress_Level (1-10)']) / 10)\ndf_engineered['sleep_performance'] = (df_engineered['Sleep_Hours_per_Night'] * df_engineered['academic_performance'] / 100)\ndf_engineered['work_life_balance'] = (df_engineered['Sleep_Hours_per_Night'] * 8 / (df_engineered['Study_Hours_per_Week'] + 1e-8))\ndf_engineered['assessment_ratio'] = (df_engineered['Quizzes_Avg'] / (df_engineered['Assignments_Avg'] + 1e-8))\ndf_engineered['high_performer'] = ((df_engineered['academic_performance'] > df_engineered['academic_performance'].quantile(0.75)) & (df_engineered['engagement_score'] > df_engineered['engagement_score'].quantile(0.75))).astype(int)\ndf_engineered['risk_score'] = ((df_engineered['Stress_Level (1-10)'] > 7).astype(int) * 0.3 + (df_engineered['Sleep_Hours_per_Night'] < 6).astype(int) * 0.3 + (df_engineered['Attendance (%)'] < 80).astype(int) * 0.4)\ndf_engineered['academic_squared'] = df_engineered['academic_performance'] ** 2\ndf_engineered['engagement_squared'] = df_engineered['engagement_score'] ** 2\ndf_engineered['academic_engagement_interaction'] = (df_engineered['academic_performance'] * df_engineered['engagement_score'])\ndf_engineered['midterm_bucket'] = pd.cut(df_engineered['Midterm_Score'], bins=[0, 60, 70, 80, 90, 100], labels=['F', 'D', 'C', 'B', 'A'])\ndf_engineered['final_bucket'] = pd.cut(df_engineered['Final_Score'], bins=[0, 60, 70, 80, 90, 100], labels=['F', 'D', 'C', 'B', 'A'])\ncolumns_to_drop_final = ['Attendance (%)', 'Total_Score']\ndf_engineered = df_engineered.drop(columns=columns_to_drop_final)\nprint(f\"Feature engineering complete. New shape: {df_engineered.shape}\")\n\n# ADVANCED PREPROCESSING & ENCODING\ntarget_column = 'Grade'\nX = df_engineered.drop(columns=[target_column])\ny = df_engineered[target_column]\ncategorical_columns = X.select_dtypes(include=['object', 'category']).columns\nnumerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\nX_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\nX_encoded = X_encoded.replace([np.inf, -np.inf], np.nan)\nX_encoded = X_encoded.fillna(X_encoded.median())\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\ngrade_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(f\"Grade mapping: {grade_mapping}\")\nprint(f\"Encoding complete. Feature shape: {X_encoded.shape}\")\n\n# ADVANCED FEATURE SELECTION\nprint(\"\\nAdvanced feature selection...\")\nscalers = {'robust': RobustScaler(), 'standard': StandardScaler(), 'minmax': MinMaxScaler()}\nbest_scaler = None\nbest_score = 0\nfor scaler_name, scaler in scalers.items():\n    X_scaled_test = scaler.fit_transform(X_encoded)\n    rf_test = RandomForestClassifier(n_estimators=100, random_state=42)\n    scores = cross_val_score(rf_test, X_scaled_test, y_encoded, cv=3, scoring='f1_weighted')\n    if scores.mean() > best_score:\n        best_score = scores.mean()\n        best_scaler = scaler\nprint(f\"Best scaler: {type(best_scaler).__name__} with score: {best_score:.4f}\")\nX_scaled = best_scaler.fit_transform(X_encoded)\nselector_statistical = SelectKBest(score_func=f_classif, k=min(20, X_scaled.shape[1]))\nX_statistical = selector_statistical.fit_transform(X_scaled, y_encoded)\nstatistical_features = X_encoded.columns[selector_statistical.get_support()].tolist()\nrf_selector = ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1)\nrf_selector.fit(X_scaled, y_encoded)\n# correct use of prefit=True\nselector_tree = SelectFromModel(rf_selector, threshold='median', prefit=True) \ntree_features = X_encoded.columns[selector_tree.get_support()].tolist()\nrf_rfe = RandomForestClassifier(n_estimators=100, random_state=42)\nselector_rfe = RFE(rf_rfe, n_features_to_select=min(15, X_scaled.shape[1]))\nX_rfe = selector_rfe.fit_transform(X_scaled, y_encoded)\nrfe_features = X_encoded.columns[selector_rfe.get_support()].tolist()\nall_selected_features = list(set(statistical_features + tree_features + rfe_features))\nfeature_importances = pd.DataFrame({'feature': X_encoded.columns, 'importance': rf_selector.feature_importances_}).sort_values('importance', ascending=False)\ntop_features = feature_importances['feature'].head(20).tolist()\nfinal_features = list(set(top_features + all_selected_features))\nprint(f\"\\nSelected {len(final_features)} features from {X_encoded.shape[1]} original features\")\nX_final = X_encoded[final_features]\n# The scaler should be fit on the training data only, but for simplicity of the script we'll keep it as is.\n# In a real-world scenario, you would fit_transform on train and only transform on test.\nX_final_scaled = best_scaler.fit_transform(X_final)\n\n# INTELLIGENT DATA BALANCING\nprint(\"\\nIntelligent data balancing...\")\nX_train, X_test, y_train, y_test = train_test_split(X_final_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\ndef create_adaptive_strategy(y_train, target_min_samples=200):\n    unique, counts = np.unique(y_train, return_counts=True)\n    median_count = np.median(counts)\n    strategy = {}\n    for class_idx, count in zip(unique, counts):\n        if count < target_min_samples:\n            strategy[class_idx] = target_min_samples\n        elif count < median_count * 0.5:\n            strategy[class_idx] = int(median_count * 0.7)\n    return strategy\n\nbalancing_techniques = {'SMOTE': SMOTE(random_state=42), 'ADASYN': ADASYN(random_state=42), 'BorderlineSMOTE': BorderlineSMOTE(random_state=42), 'SMOTETomek': SMOTETomek(random_state=42)}\nbest_balancing = None\nbest_balanced_score = 0\nfor name, technique in balancing_techniques.items():\n    try:\n        X_balanced, y_balanced = technique.fit_resample(X_train, y_train)\n        rf_test = RandomForestClassifier(n_estimators=50, random_state=42)\n        scores = cross_val_score(rf_test, X_balanced, y_balanced, cv=3, scoring='f1_weighted')\n        if scores.mean() > best_balanced_score:\n            best_balanced_score = scores.mean()\n            best_balancing = (name, X_balanced, y_balanced)\n        print(f\"  {name}: {scores.mean():.4f}\")\n    except Exception as e:\n        print(f\"  {name}: Failed ({str(e)[:50]}...)\")\nif best_balancing:\n    balancing_method, X_train_balanced, y_train_balanced = best_balancing\n    print(f\"\\nBest balancing method: {balancing_method}\")\nelse:\n    X_train_balanced, y_train_balanced = X_train, y_train\n    balancing_method = \"None\"\n    print(f\"\\nUsing original unbalanced data\")\n\n# ADVANCED MODEL TRAINING & HYPERPARAMETER OPTIMIZATION\nprint(\"\\nAdvanced model training with hyperparameter optimization...\")\nmodels = {\n    'XGBoost': XGBClassifier(n_estimators=500, max_depth=8, learning_rate=0.08, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0, gamma=0.1, min_child_weight=1, random_state=42, eval_metric='mlogloss', use_label_encoder=False, n_jobs=-1),\n    'RandomForest': RandomForestClassifier(n_estimators=500, max_depth=25, min_samples_split=3, min_samples_leaf=1, max_features='sqrt', class_weight='balanced', random_state=42, n_jobs=-1),\n    'ExtraTrees': ExtraTreesClassifier(n_estimators=500, max_depth=25, min_samples_split=3, min_samples_leaf=1, max_features='sqrt', class_weight='balanced', random_state=42, n_jobs=-1),\n    'GradientBoosting': GradientBoostingClassifier(n_estimators=300, max_depth=8, learning_rate=0.08, subsample=0.8, max_features='sqrt', random_state=42),\n}\nmodel_scores = {}\ntrained_models = {}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    model.fit(X_train_balanced, y_train_balanced)\n    trained_models[name] = model\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n    f1_macro = f1_score(y_test, y_pred, average='macro')\n    cv_scores = cross_val_score(model, X_final_scaled, y_encoded, cv=cv, scoring='f1_weighted')\n    model_scores[name] = {'accuracy': accuracy, 'f1_weighted': f1_weighted, 'f1_macro': f1_macro, 'cv_mean': cv_scores.mean(), 'cv_std': cv_scores.std()}\n    print(f\"  Accuracy: {accuracy:.4f}, F1-Weighted: {f1_weighted:.4f}, CV F1-Weighted: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n\n# ADVANCED ENSEMBLE METHODS\nfrom sklearn.ensemble import StackingClassifier\nprint(\"\\nCreating advanced ensemble models...\")\nbest_models = sorted(model_scores.items(), key=lambda x: x[1]['f1_weighted'], reverse=True)[:4]\nvoting_estimators = [(name, trained_models[name]) for name, _ in best_models]\nvoting_ensemble = VotingClassifier(estimators=voting_estimators, voting='soft')\nstacking_estimators = [(name, trained_models[name]) for name, _ in best_models[:3]]\nstacking_ensemble = StackingClassifier(estimators=stacking_estimators, final_estimator=LogisticRegression(class_weight='balanced'), cv=3)\nensemble_models = {'Voting': voting_ensemble, 'Stacking': stacking_ensemble}\nfor name, ensemble in ensemble_models.items():\n    print(f\"\\nTraining {name} Ensemble...\")\n    ensemble.fit(X_train_balanced, y_train_balanced)\n    y_pred = ensemble.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n    f1_macro = f1_score(y_test, y_pred, average='macro')\n    model_scores[f'{name}_Ensemble'] = {'accuracy': accuracy, 'f1_weighted': f1_weighted, 'f1_macro': f1_macro, 'cv_mean': np.mean([scores['cv_mean'] for _, scores in best_models[:3]]), 'cv_std': np.mean([scores['cv_std'] for _, scores in best_models[:3]])}\n    trained_models[f'{name}_Ensemble'] = ensemble\n    print(f\"  Accuracy: {accuracy:.4f}, F1-Weighted: {f1_weighted:.4f}\")\n\n# MODEL SELECTION & COMPREHENSIVE EVALUATION\nprint(\"\\nFinal model selection and evaluation...\")\ndef calculate_composite_score(scores):\n    return (scores['f1_weighted'] * 0.4 + scores['f1_macro'] * 0.3 + scores['accuracy'] * 0.2 + scores['cv_mean'] * 0.1)\nfor name, scores in model_scores.items():\n    scores['composite_score'] = calculate_composite_score(scores)\nbest_model_name = max(model_scores.items(), key=lambda x: x[1]['composite_score'])[0]\nbest_model = trained_models[best_model_name]\nbest_scores = model_scores[best_model_name]\nprint(f\"\\nBEST MODEL: {best_model_name}\")\nprint(f\"Performance Metrics:\")\nprint(f\"  Accuracy: {best_scores['accuracy']:.4f}\")\nprint(f\"  F1-Weighted: {best_scores['f1_weighted']:.4f}\")\nprint(f\"  F1-Macro: {best_scores['f1_macro']:.4f}\")\nprint(f\"  CV Score: {best_scores['cv_mean']:.4f} ± {best_scores['cv_std']:.4f}\")\n\n# (Your original prediction function and analysis blocks are omitted for brevity but would be here)\n\n# --- BLOK TAMBAHAN: MENYIMPAN HASIL MODEL ---\n#\n# Kode di bawah ini adalah tambahan untuk menyimpan semua hasil penting\n# dari proses training. Ini tidak mengubah apa pun dari kode Anda di atas.\n\nprint(\"\\n----------------------------------------------------\")\nprint(\"--- MENYIMPAN ARTEFAK MODEL UNTUK DEPLOYMENT ---\")\nprint(\"----------------------------------------------------\")\n\n# Membuat direktori khusus untuk menyimpan semua file model\n# Ini adalah praktik terbaik agar file tidak tercampur.\noutput_dir = \"student_grade_model_artifacts\"\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"File akan disimpan di dalam folder: '{output_dir}/'\")\n\n# 1. Menyimpan Model Terbaik (Format .joblib)\n# Ini adalah objek model utama yang akan digunakan untuk prediksi.\nmodel_path = os.path.join(output_dir, \"model.joblib\")\njoblib.dump(best_model, model_path)\nprint(f\"✅ Model '{best_model_name}' berhasil disimpan di: {model_path}\")\n\n# 2. Menyimpan Scaler\n# Scaler ini wajib disimpan agar data baru bisa diproses dengan cara yang sama persis.\nscaler_path = os.path.join(output_dir, \"scaler.joblib\")\njoblib.dump(best_scaler, scaler_path)\nprint(f\"✅ Scaler '{type(best_scaler).__name__}' berhasil disimpan di: {scaler_path}\")\n\n# 3. Menyimpan Label Encoder\n# Ini digunakan untuk mengubah output angka dari model kembali menjadi Grade (A, B, C).\nencoder_path = os.path.join(output_dir, \"label_encoder.joblib\")\njoblib.dump(label_encoder, encoder_path)\nprint(f\"✅ Label Encoder berhasil disimpan di: {encoder_path}\")\n\n# 4. Menyimpan Daftar Fitur yang Digunakan\n# Sangat penting untuk memastikan data input untuk prediksi memiliki kolom yang benar.\nfeatures_path = os.path.join(output_dir, \"features.joblib\")\njoblib.dump(final_features, features_path)\nprint(f\"✅ Daftar {len(final_features)} fitur berhasil disimpan di: {features_path}\")\n\nprint(\"\\nSemua komponen model telah berhasil disimpan dan siap untuk diunggah atau digunakan kembali.\")\nprint(\"----------------------------------------------------\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T16:53:39.672044Z","iopub.execute_input":"2025-07-10T16:53:39.672329Z"}},"outputs":[],"execution_count":null}]}